---
title: "Korpus native tekstova"
author: "Lux"
date: "2023-03-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r echo=F, eval=T, message=F , warning= FALSE, message=F}
library(tidyverse)
library(readxl)
library(xlsx)
library(here)
library(kableExtra)
library(DT)
library(purrr)
library(data.table)
library(tidytext)
library(dplyr)
library(lubridate)
library(anytime)
library(grid)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
library(topicmodels)
library(ggthemes)
library(xlsx)
library(knitr)
library(kableExtra)
library(stopwords)
library(here)

```



```{r echo=F, eval=T, message=F , warning= FALSE}
source("./Source/stemmer.R")
source("./Source/text_analysis.R")
```


```{r echo=F, eval=T, message=F , warning= FALSE}
dt1 <- fread("./Data/channel1.csv" , encoding = "UTF-8")
dt2 <- fread("./Data/channel2.csv")
dt3 <- fread("./Data/channel3.csv")
dt4 <- fread("./Data/channel4.csv")

dt <- rbind(dt1, dt2, dt3, dt4)

```

#### Deskriptiva na dnevnoj razini
```{r echo=F, eval=T, message=F , warning= FALSE}

number_of_videos <- dt %>%
  group_by(channelTitle) %>%
  summarise(count = n()) %>%
  arrange(desc(count))


number_of_videos

```



```{r echo=F, eval=T, message=F , warning= FALSE}

# Convert publishedAtSQL to Date type (if not already in date format)
dt <- dt %>%
  mutate(publishedAt = as.Date(publishedAt))

# Arrange the data by channelTitle and publication date, and compute cumulative count
dt_cumulative <- dt %>%
  group_by(channelTitle) %>%
  arrange(publishedAt) %>%
  mutate(cumulative_videos = row_number())

# Plot using ggplot2 with facets for each channelTitle
ggplot(dt_cumulative, aes(x = publishedAt, y = cumulative_videos)) +
  geom_line(size = 1) +  # Line plot for cumulative videos
  labs(title = "Cumulative Number of Videos Over Time by Channel",
       x = "Date",
       y = "Cumulative Number of Videos") +
  facet_wrap(~ channelTitle, ncol = 2) +  # Create a facet for each channel (2 columns layout)
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for readability
```


```{r echo=F, eval=T, message=F , warning= FALSE}
# Compute descriptive statistics grouped by channelTitle
dt_stats <- dt %>%
  group_by(channelTitle) %>%
  summarize(
    mean_duration = mean(durationSec, na.rm = TRUE),
    median_duration = median(durationSec, na.rm = TRUE),
    min_duration = min(durationSec, na.rm = TRUE),
    max_duration = max(durationSec, na.rm = TRUE),
    sd_duration = sd(durationSec, na.rm = TRUE),
    
    mean_likes = mean(likeCount, na.rm = TRUE),
    median_likes = median(likeCount, na.rm = TRUE),
    min_likes = min(likeCount, na.rm = TRUE),
    max_likes = max(likeCount, na.rm = TRUE),
    sd_likes = sd(likeCount, na.rm = TRUE),
    
    mean_comments = mean(commentCount, na.rm = TRUE),
    median_comments = median(commentCount, na.rm = TRUE),
    min_comments = min(commentCount, na.rm = TRUE),
    max_comments = max(commentCount, na.rm = TRUE),
    sd_comments = sd(commentCount, na.rm = TRUE),
    
    mean_views = mean(viewCount, na.rm = TRUE),
    median_views = median(viewCount, na.rm = TRUE),
    min_views = min(viewCount, na.rm = TRUE),
    max_views = max(viewCount, na.rm = TRUE),
    sd_views = sd(viewCount, na.rm = TRUE)
  ) %>%
  mutate_if(is.numeric, round, digits = 0)  # Round the results to 2 decimal places

# Display the result dynamically using the DT package

datatable(dt_stats, rownames = FALSE, options = list(pageLength = 5))

```


# Text analysis 

```{r echo=F, eval=T, message=F , warning= FALSE}
# read in lexicons
CroSentilex_n <- read.delim("C:/Users/Lukas/Dropbox/Mislav@Luka/crosentilex-negatives.txt",
                                   header = FALSE,
                                   sep = " ",
                                   stringsAsFactors = FALSE,
                                   fileEncoding = "UTF-8")  %>%
                   rename(word = "V1", sentiment = "V2" ) %>%
                   mutate(brija = "NEG")
 
CroSentilex_p  <- read.delim("C:/Users/Lukas/Dropbox/Mislav@Luka/crosentilex-positives.txt",
                                   header = FALSE,
                                   sep = " ",
                                   stringsAsFactors = FALSE,
                                   fileEncoding = "UTF-8") %>%
                    rename(word = "V1", sentiment = "V2" ) %>%
                    mutate(brija = "POZ")
 
Crosentilex_sve <- rbind(setDT(CroSentilex_n), setDT(CroSentilex_p))
# check lexicon data 
#head(sample_n(Crosentilex_sve,1000),15)

 
CroSentilex_Gold  <- read.delim2("C:/Users/Lukas/Dropbox/Mislav@Luka/gs-sentiment-annotations.txt",
                                 header = FALSE,
                                 sep = " ",
                                 stringsAsFactors = FALSE) %>%
                    rename(word = "V1", sentiment = "V2" ) 
 Encoding(CroSentilex_Gold$word) <- "UTF-8"
 CroSentilex_Gold[1,1] <- "dati"
 CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "-", "1")
 CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "\\+", "2")
 CroSentilex_Gold$sentiment <- as.numeric(unlist(CroSentilex_Gold$sentiment))
# check lexicon data 
#head(sample_n(CroSentilex_Gold,100),15)

 
LilaHR  <- read_excel("C:/Users/Lukas/Dropbox/Mislav@Luka/lilaHR_clean.xlsx", sheet = "Sheet1") %>% select (-"...1")
LilaHR_long <- read_excel("C:/Users/Lukas/Dropbox/Mislav@Luka/lilaHR_clean_long.xlsx", sheet = "Sheet1") %>% select (-"...1") 



# Print the long format data
#print(data_long)

#proba <- read.csv2("C:/Users/Lukas/Dropbox/Mislav@Luka/lilaHRcsv.csv", encoding = "UTF-8")
#df <- separate_rows(LilaHR, HR, sep = ", ") 
# 
# zero_rows_count <- sum(apply(df[-1], 1, function(row) all(row == 0)))
# print(zero_rows_count)
# 
# filtered_df <- df %>% 
#   filter(!apply(.[,-1], 1, function(row) all(row == 0)))
#  
# write.xlsx(filtered_df, "C:/Users/Lukas/Dropbox/Mislav@Luka/lilaHR_.xlsx" )

  
# create stop words
stopwords_cro <- get_stopwords(language = "hr", source = "stopwords-iso")
# check stopwords data
#head(sample_n(stopwords_cro,100),15)
# extend stop words
my_stop_words <- tibble(
  word = c(
    "jedan","mjera", "može", "možete", "mogu", "kad", "sada", "treba", "ima", "osoba",
    "e","prvi", "dva","dvije","drugi",
    "tri","treći","pet","kod",
    "ove","ova",  "ovo","bez", "kod",
    "evo","oko",  "om", "ek",
    "mil","tko","šest", "sedam",
    "osam",   "čim", "zbog",
    "prema", "dok","zato", "koji", 
    "im", "čak","među", "tek",
    "koliko", "tko","kod","poput", 
    "baš", "dakle", "osim", "svih", 
    "svoju", "odnosno", "gdje",
    "kojoj", "ovi", "toga",
     "ubera", "vozača", "hrvatskoj", "usluge", "godine", "više", "taksi", "taxi", "taksija", "taksija", "kaže", "rekao", "19"," aee", "ae","bit.ly", "https", "one", "the"
  ),
  lexicon = "lux"
)

# full set with diacritics
cro_sw_full_d <- tibble(word = c("a","ako","ali","baš","bez","bi","bih","bila","bili","bilo","bio","bismo","bit","biti","bolje","bude","čak","čega","čemu","često","četiri","čime","čini","će","ćemo","ćete","ću","da","dakle","dalje","dan","dana","dana","danas","dio","do","dobro","dok","dosta","dva","dvije","eto","evo","ga","gdje","god","godina","godine","gotovo","grada","i","iako","ići","ih","ili","im","ima","imaju","imali","imam","imao","imati","inače","ipak","isto","iz","iza","između","ja","jako","je","jedan","jedna","jednog","jednom","jednostavno","jednu","jer","joj","još","ju","ka","kad","kada","kaj","kako","kao","kaže","kod","koja","koje","kojeg","kojeg","kojem","koji","kojih","kojim","kojima","kojoj","kojom","koju","koliko","kraju","kroz","li","malo","manje","me","među","međutim","mene","meni","mi","milijuna","mislim","mjesto","mnogo","mogao","mogli","mogu","moj","mora","možda","može","možemo","možete","mu","na","način","nad","naime","nakon","nam","naravno","nas","ne","neće","nego","neka","neke","neki","nekog","nekoliko","neku","nema","nešto","netko","ni","nije","nikad","nisam","nisu","ništa","niti","no","njih","o","od","odmah","odnosno","oko","on","ona","onda","oni","onih","ono","opet","osim","ova","ovaj","ovdje","ove","ovim","ovo","ovog","ovom","ovu","pa","pak","par","po","pod","poput","posto","postoji","pred","preko","prema","pri","prije","protiv","prvi","puno","put","radi","reći","s","sa","sad","sada","sam","samo","sati","se","sebe","si","smo","ste","stoga","strane","su","svaki","sve","svi","svih","svoj","svoje","svoju","što","ta","tada","taj","tako","također","tamo","te","tek","teško","ti","tih","tijekom","time","tko","to","tog","toga","toj","toliko","tom","tome","treba","tu","u","uopće","upravo","uvijek","uz","vam","vas","već","vi","više","vrijeme","vrlo","za","zapravo","zar","zato","zbog","zna","znači"),
                        lexicon = "boras")


stop_corpus <- my_stop_words %>%
  bind_rows(stopwords_cro)


stop_corpus <- stop_corpus %>%
  bind_rows(cro_sw_full_d)

# check stopwords data
#head(sample_n(stop_corpus,100),15)
```


```{r echo=F, eval=T, message=F , warning= FALSE}
# dim before tokenize
#dim(dta)

# tokenize
dt %>% 
  unnest_tokens(word, videoTitle) -> n_token

# dim after tokenize
#dim(n_token)

# check
# fb_token %>% 
#   select(FROM, word, MENTION_SNIPPET ) %>%
#     sample_n(.,100)

# remove stop words, numbers, single letters
n_token %>% 
  anti_join(stop_corpus, by = "word") %>%
  mutate(word = gsub("\\d+", NA, word)) %>%
  mutate(word = gsub("^[a-zA-Z]$", NA, word)) -> n_tokenTidy
# remove NA
n_tokenTidy %>%
  filter(!is.na(word)) -> n_tokenTidy


n_tokenTidy %>%
   mutate(stem = sapply(word, write_tokens))  %>%
        mutate(stem = sapply(strsplit(stem, "\t"), `[`, 2)) -> n_tokenTidy

# check
# fb_tokenTidy  %>% 
#   select(FROM, word, MENTION_SNIPPET ) %>%
#   sample_n(.,100)

# dim after clean
#dim(n_tokenTidy)

```










#### Najčešće riječi (video title)
```{r echo=F, eval=T, message=F , warning= FALSE}
n_tokenTidy %>%
  group_by(stem) %>%
  summarise(count = n()) %>%
  mutate(percent = round(count / sum(count) * 100,2)) %>% 
  arrange(desc(count)) %>%
  filter(count > 10) %>%
  datatable(., options = list(scrollX = TRUE, scrollY = "500px"))

```

```{r echo=F, eval=T, message=F , warning= FALSE, fig.height=15, fig.width=15}
# Filter, group, and count words for all channelTitles
word_freq_data <- n_tokenTidy %>%
  filter(channelTitle %in% c("Anita Luis Osobni razvoj", 
                             "Bojana Svalina", 
                             "SAFARI DUHA", 
                             "Put u središte sebe")) %>%
  group_by(channelTitle, stem) %>%
  count() %>%
  filter(n > 10) %>%
  ungroup()

# Create a plot with facets for each channelTitle
ggplot(word_freq_data, aes(x = reorder(stem, n), y = n)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ channelTitle, scales = "free_y") +  # Facet by channelTitle
  labs(title = "Najčešće riječi u video title po kanalu", 
       x = "Riječ", 
       y = "Broj pojavljivanja") +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 8))
```



```{r echo=F, eval=T, message=F , warning= FALSE}

# proba <- CroSentilex_Gold %>%
# #  slice(1:500) %>%
#   mutate(
#     results = map(word, write_tokens),
#     korijen = map_chr(results, ~ str_extract(.x, "(?<=\t)[^\t]+$")),
#     rijec = map_chr(results, ~ str_extract(.x, "^[^\t]+(?=\t)"))
#   ) %>%
#   select(-results)
# CroSentilex_Gold <- proba %>% select(-"word") %>% rename("word" ="korijen")


## Sentiment 
doprinos_sentimentu <- function(dataset, no = n) {
dataset %>%
  inner_join(CroSentilex_Gold, by = "word") %>% 
  count(word, sentiment,sort = TRUE) %>% 
  group_by(sentiment) %>%
  top_n(no) %>%
  ungroup() %>%
  mutate(sentiment = case_when(sentiment == 0 ~ "NEUTRALNO",
                                 sentiment == 1 ~ "NEGATIVNO",
                                 sentiment == 2 ~ "POZITIVNO")) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  ggtitle( "Sentiment") +
  labs( x = "Riječ", y = "Number of words") +
  facet_wrap(~ sentiment, scales = "free_y") +
  coord_flip() +
  scale_fill_manual(values = c("grey40", "grey50","grey60")) +  # Assuming two sentiment values; adjust as needed
  theme_minimal() + 
  theme(
    panel.background = element_blank(),
    strip.background = element_blank(),
    panel.grid = element_blank()
  ) -> gg_doprinos_sentimentu
  
 gg_doprinos_sentimentu
 
}
doprinos_sentimentu(n_tokenTidy,30)



```



```{r echo=F, eval=T, message=F , warning= FALSE}
fb_bigram <- dt %>%
  unnest_tokens(bigram, videoDescription, token = "ngrams", n = 2)
#fb_bigram %>% head(10)
# fb_bigram %>%
#   count(bigram, sort = T) %>%
#   head(25) 
fb_bigram_sep <- fb_bigram %>%
  separate(bigram, c("word1","word2"), sep = " ")
fb_bigram_tidy <- fb_bigram_sep %>%
  filter(!word1 %in% stop_corpus$word) %>%
  filter(!word2 %in% stop_corpus$word) %>%
  mutate(word1 = gsub("\\d+", NA, word1)) %>%
  mutate(word2 = gsub("\\d+", NA, word2)) %>%
  mutate(word1 = gsub("^[a-zA-Z]$", NA, word1)) %>%
  mutate(word2 = gsub("^[a-zA-Z]$", NA, word2)) 
fb_bigram_tidy_bigram_counts <- fb_bigram_tidy %>% 
  count(word1, word2, sort = TRUE)

bigrams_united <- fb_bigram_tidy %>%
  unite(bigram, word1, word2, sep = " ") %>%
  filter(., !grepl("NA",bigram))
#bigrams_united
bigrams_united %>% 
  count(channelTitle,bigram,sort = T) -> topicBigram

bigrams_united %>%
  count(bigram, sort = T) %>%
  filter(n>1) %>%
  datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
```


```{r echo=F, eval=T, message=F , warning= FALSE}
fb_bigram <- dt %>%
  unnest_tokens(bigram, videoDescription, token = "ngrams", n = 3)
#fb_bigram %>% head(10)
# fb_bigram %>%
#   count(bigram, sort = T) %>%
#   head(25) 
fb_bigram_sep <- fb_bigram %>%
  separate(bigram, c("word1","word2"), sep = " ")
fb_bigram_tidy <- fb_bigram_sep %>%
  filter(!word1 %in% stop_corpus$word) %>%
  filter(!word2 %in% stop_corpus$word) %>%
  mutate(word1 = gsub("\\d+", NA, word1)) %>%
  mutate(word2 = gsub("\\d+", NA, word2)) %>%
  mutate(word1 = gsub("^[a-zA-Z]$", NA, word1)) %>%
  mutate(word2 = gsub("^[a-zA-Z]$", NA, word2)) 
fb_bigram_tidy_bigram_counts <- fb_bigram_tidy %>% 
  count(word1, word2, sort = TRUE)

bigrams_united <- fb_bigram_tidy %>%
  unite(bigram, word1, word2, sep = " ") %>%
  filter(., !grepl("NA",bigram))
#bigrams_united
bigrams_united %>% 
  count(channelTitle,bigram,sort = T) -> topicBigram

bigrams_united %>%
  count(bigram, sort = T) %>%
  filter(n>1) %>%
  datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
```





#### Tematska analiza


```{r eval = F, echo = F, message=F, warning=F, fig.height=15, fig.width=15}

dtm <- n_tokenTidy %>%
  count(document, word) %>%
  cast_dtm(document, word, n)

# Perform Latent Dirichlet Allocation (LDA) for topic modeling
# Here, k = number of topics. You can adjust this based on your data
lda_model2 <- LDA(dtm, k = 2, control = list(seed = 1234))

# Tidy the LDA output
lda_topics <- tidy(lda_model2, matrix = "beta")

# Get the top terms for each topic
top_terms <- lda_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 15) %>% # Adjust 'n' for the number of terms you want to show
  ungroup() %>%
  arrange(topic, -beta)

# Print the top terms for each topic
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "",
       x = NULL, y = "Beta")




```


```{r eval = F, echo = F, message=F, warning=F, fig.height=15, fig.width=15}
lda_model3 <- LDA(dtm, k = 3, control = list(seed = 1234))

# Tidy the LDA output
lda_topics <- tidy(lda_model3, matrix = "beta")

# Get the top terms for each topic
top_terms <- lda_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 15) %>% # Adjust 'n' for the number of terms you want to show
  ungroup() %>%
  arrange(topic, -beta)

# Print the top terms for each topic
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "",
       x = NULL, y = "Beta")

```







#### Najčešće riječi (video description)

```{r echo=F, eval=T, message=F , warning= FALSE}


# tokenize
dt %>% 
  unnest_tokens(word, videoDescription) -> n_token

# dim after tokenize
#dim(n_token)

# check
# fb_token %>% 
#   select(FROM, word, MENTION_SNIPPET ) %>%
#     sample_n(.,100)

# remove stop words, numbers, single letters
n_token %>% 
  anti_join(stop_corpus, by = "word") %>%
  mutate(word = gsub("\\d+", NA, word)) %>%
  mutate(word = gsub("^[a-zA-Z]$", NA, word)) -> n_tokenTidy
# remove NA
n_tokenTidy %>%
  filter(!is.na(word)) -> n_tokenTidy

# check
# fb_tokenTidy  %>% 
#   select(FROM, word, MENTION_SNIPPET ) %>%
#   sample_n(.,100)

# dim after clean
#dim(n_tokenTidy)




n_tokenTidy %>%
  group_by(word) %>%
  summarise(count = n()) %>%
  mutate(percent = round(count / sum(count) * 100,2)) %>% 
  arrange(desc(count)) %>%
  filter(count > 10) %>%
  datatable(., options = list(scrollX = TRUE, scrollY = "500px"))

```







```{r echo=F, eval=T, message=F , warning= FALSE}

# proba <- CroSentilex_Gold %>%
# #  slice(1:500) %>%
#   mutate(
#     results = map(word, write_tokens),
#     korijen = map_chr(results, ~ str_extract(.x, "(?<=\t)[^\t]+$")),
#     rijec = map_chr(results, ~ str_extract(.x, "^[^\t]+(?=\t)"))
#   ) %>%
#   select(-results)
# CroSentilex_Gold <- proba %>% select(-"word") %>% rename("word" ="korijen")


## Sentiment 
doprinos_sentimentu <- function(dataset, no = n) {
dataset %>%
  inner_join(CroSentilex_Gold, by = "word") %>% 
  count(word, sentiment,sort = TRUE) %>% 
  group_by(sentiment) %>%
  top_n(no) %>%
  ungroup() %>%
  mutate(sentiment = case_when(sentiment == 0 ~ "NEUTRALNO",
                                 sentiment == 1 ~ "NEGATIVNO",
                                 sentiment == 2 ~ "POZITIVNO")) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  ggtitle( "Sentiment") +
  labs( x = "Riječ", y = "Number of words") +
  facet_wrap(~ sentiment, scales = "free_y") +
  coord_flip() +
  scale_fill_manual(values = c("grey40", "grey50","grey60")) +  # Assuming two sentiment values; adjust as needed
  theme_minimal() + 
  theme(
    panel.background = element_blank(),
    strip.background = element_blank(),
    panel.grid = element_blank()
  ) -> gg_doprinos_sentimentu
  
 gg_doprinos_sentimentu
 
}
doprinos_sentimentu(n_tokenTidy,30)



```


##### Doprinos riječi sentimentu (NRC)

```{r echo=F, eval=T, message=F , warning= FALSE ,fig.width=16, fig.height=25}
NRCpn <- LilaHR_long %>% rename("word" = "rijec") %>%
  filter(Emotion %in% c("Positive","Negative")) %>%
  mutate(Emotion = recode(Emotion,
                          "Positive" = "Pozitivno",
                          "Negative" = "Negativno"))


## Sentiment 
doprinos_sentimentu <- function(dataset, no = n) {
dataset %>%
  inner_join(NRCpn, by = "word") %>% 
  count(word, Emotion,sort = TRUE) %>% 
  group_by(Emotion) %>%
  top_n(no) %>%
  ungroup() %>%
#  mutate(sentiment = case_when(sentiment == 0 ~ "NEUTRAL",
#                                 sentiment == 1 ~ "NEGATIVE",
#                                 sentiment == 2 ~ "POSITIVE")) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = Emotion)) +
  geom_col(show.legend = FALSE) +
  ggtitle( "Sentiment") +
  labs( x = "Riječ", y = "Broj riječi") +
  facet_wrap(~ Emotion, scales = "free_y") +
  coord_flip() +
  scale_fill_manual(values = c("grey40", "grey50")) +  # Assuming two sentiment values; adjust as needed
  theme_minimal() + 
  theme(
    panel.background = element_blank(),
    strip.background = element_blank(),
    panel.grid = element_blank()
  ) -> gg_doprinos_sentimentu
  
 gg_doprinos_sentimentu
 
}




doprinos_sentimentu(n_tokenTidy,30)

```


##### Doprinos riječi raznom sentimentu (NRC)
```{r echo=F, eval=T, message=F , warning= FALSE, fig.width=16, fig.height=25}


NRC <- LilaHR_long %>% rename("word" = "rijec") %>%
  filter(Emotion %in% c("Anger","Anticipation","Disgust","Fear","Joy","Sadness","Surprise","Trust")) %>%
  mutate(Emotion = recode(Emotion,
                          "Anger" = "Ljutnja",
                          "Anticipation" = "Iščekivanje",
                          "Disgust" = "Gađenje",
                          "Fear" = "Strah",
                          "Joy" = "Zadovoljstvo",
                          "Sadness" = "Tuga",
                          "Surprise" = "Iznenađenje",
                          "Trust" = "Povjerenje"))


## Sentiment 
doprinos_sentimentu_full <- function(dataset, no = n) {
dataset %>%
  inner_join(NRC, by = "word") %>% 
  count(word, Emotion,sort = TRUE) %>% 
  group_by(Emotion,) %>%
  top_n(no) %>%
  ungroup() %>%
#  mutate(sentiment = case_when(sentiment == 0 ~ "NEUTRAL",
#                                 sentiment == 1 ~ "NEGATIVE",
#                                 sentiment == 2 ~ "POSITIVE")) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = Emotion)) +
  geom_col(show.legend = FALSE) +
  ggtitle( "Sentiment") +
  labs( x = "Riječ", y = "Broj riječi") +
  facet_wrap(~ Emotion, scales = "free_y") +
  coord_flip() +
  scale_fill_manual(values = c("grey10", "grey20","grey30","grey40","grey50","grey60","grey70","grey80")) +  # Assuming two sentiment values; adjust as needed
  theme_minimal() + 
  theme(
    panel.background = element_blank(),
    strip.background = element_blank(),
    panel.grid = element_blank()
  ) -> gg_doprinos_sentimentu
  
 gg_doprinos_sentimentu
 
}
doprinos_sentimentu_full(n_tokenTidy,20)
```



#### Oblak riječi sa sentimentom CroSentilex

```{r echo=F, eval=T, message=F , warning= FALSE}
## ComparisonCloud
n_tokenTidy %>%
  inner_join(CroSentilex_Gold,by="word") %>% 
  count(word, sentiment) %>% 
  top_n(200) %>%
  mutate(sentiment = case_when(sentiment == 0 ~ "+/-",
                                 sentiment == 1 ~ "-",
                                 sentiment == 2 ~ "+")) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("firebrick3", "deepskyblue3","darkslategray"),
                   max.words = 120)

```

#### Oblak riječi sa sentimentom NRC


```{r echo=F, eval=T, message=F , warning= FALSE}
n_tokenTidy %>%
  inner_join(NRCpn,by="word") %>% 
  count(word, Emotion) %>% 
  top_n(200) %>%
#  mutate(sentiment = case_when(sentiment == 0 ~ "+/-",
#                                sentiment == 1 ~ "-",
#                                 sentiment == 2 ~ "+")) %>%
  acast(word ~ Emotion, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("firebrick3", "deepskyblue3","darkslategray"),
                   max.words = 120)
```


```{r echo=F, eval=T, message=F , warning= FALSE}
fb_bigram <- dt %>%
  unnest_tokens(bigram, videoDescription, token = "ngrams", n = 2)
#fb_bigram %>% head(10)
# fb_bigram %>%
#   count(bigram, sort = T) %>%
#   head(25) 
fb_bigram_sep <- fb_bigram %>%
  separate(bigram, c("word1","word2"), sep = " ")
fb_bigram_tidy <- fb_bigram_sep %>%
  filter(!word1 %in% stop_corpus$word) %>%
  filter(!word2 %in% stop_corpus$word) %>%
  mutate(word1 = gsub("\\d+", NA, word1)) %>%
  mutate(word2 = gsub("\\d+", NA, word2)) %>%
  mutate(word1 = gsub("^[a-zA-Z]$", NA, word1)) %>%
  mutate(word2 = gsub("^[a-zA-Z]$", NA, word2)) 
fb_bigram_tidy_bigram_counts <- fb_bigram_tidy %>% 
  count(word1, word2, sort = TRUE)

bigrams_united <- fb_bigram_tidy %>%
  unite(bigram, word1, word2, sep = " ") %>%
  filter(., !grepl("NA",bigram))
#bigrams_united
bigrams_united %>% 
  count(channelTitle,bigram,sort = T) -> topicBigram

bigrams_united %>%
  count(bigram, sort = T) %>%
  filter(n>1) %>%
  datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
```














































































